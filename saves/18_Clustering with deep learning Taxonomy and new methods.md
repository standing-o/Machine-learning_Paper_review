## Clustering with deep learning: Taxonomy and new methods

- Authors : Aljalbout, Elie and Golkov, Vladimir and Siddiqui, Yawar and Strobel, Maximilian and Cremers, Daniel
- Journal : arXiv
- Year : 2018
- Link : https://arxiv.org/pdf/1801.07648.pdf

### Abstract
- Clustering methods based on deep neural networks âž” High representational power

### Introduction
- The main objective of clustering is to separate data into groups of similar data points.
- The performance of current clustering methods is however highly dependent on the input data. 
- Dimensionality reduction and representation learning have been extensively used alongside clustering in order to map the input data into a feature space where separation is easier. By utilizing deep neural networks, it is possible to learn non-linear mappings that allow transforming data into more clustering-friendly representations without manual feature extraction/selection.
- The general pipeline of most deep-learning-based clustering methods
  <img src="https://user-images.githubusercontent.com/57218700/228452177-cecb224a-185f-4f3b-ac19-e63b53e37505.png" width=100%>
  - Auto-encoder is trained with the standard mean squared error reconstruction loss. The auto-encoder fine-tuned with a combined loss function consisting of the auto-encoder reconstruction loss and a clustering-specific loss.

### Architecture of Neural networks
#### Multi-layer Perceptron (MLP)
- Feedforward network. The output of every hidden layer is the input to next one
#### Convolutional Neural network (CNN)
- Useful for applications to images, if locality and shift-equivariance/invariance of feature extraction is desired.
#### Deep belief network (DBN)
- Generative graphical model, consisting of several layers of latent variable.
- Composed of several shallow networks such as restricted Boltzmann machines, s.t. the hidden layer of each sub-network serves as the visible layer of the next sub-network.
#### Generative Adversarial network (GAN)
- The generator learns a distribution of interest to produce samples. The discriminator learns to distinguish between real samples and generated ones.
#### Variational Auto encoder (VAE)
- A Bayesian network with an autoencoder architecture that learns the data distribution (generative model).

### Non-clustering loss
#### No Non-clustering loss
- Non additional non-clustering loss function is used and the network model is only constrained by the clustering loss.
- Danger of worse representations/results, or theoretically even collapsing clusters

#### Auto-encoder reconstruction loss
<img src="https://latex.codecogs.com/svg.latex?\Large&space;L=d_{\text{AE}}(x_i,f(x_i))=\Sigma_i||x_i-f(x_i)||^2" width=40%>

- During training, the decoder tries to reconstruct input x from representation z in a latent space Z, making sure that useful information has not been lost by the encoding phase.
- Auto-encoders can successfully learn useful representations in the cases where the output's dimensionality is different from the input's or when random noise is injected to the input.

#### Self-augmentation loss
<img src="https://latex.codecogs.com/svg.latex?\Large&space;L=-\frac{1}{N}\Sigma_{N}s(f(x),f(T(x)))" width=30%>

- T is the augmentation function, f(x) is the representation generated by the model and s is some measure of similarity.
- A loss term pushes together the representation of the original sample and their augmentations.

### Clustering loss
#### No Clustering loss
- Even if a neural network has only non-clustering losses, the features it extracts can be used for clustering after training.

#### K-means loss
- Data points are evenly distributed around the cluster centers. To obtain such a distribution:
    <img src="https://latex.codecogs.com/svg.latex?\Large&space;L(\theta)=\Sigma^{N}_{i=1}\Sigma^{K}_{k=1}s_{ik}||z_i-\mu_k||^2" width=40%>

#### Cluster Assignment Hardening
- Requires using soft assignments of data points of clusters. Student's t-distribution can be used as the kernel to measure the similarity between points and centroids. This distribution Q:
    <img src="https://latex.codecogs.com/svg.latex?\Large&space;q_{ij}=\frac{(1+||z_i-\mu_j||^2/v)^{-\frac{v+1}{2}}}{\Sigma_{j'}(1+||z_i-\mu_{j'}||^2/v)^{-\frac{v+1}{2}}}" width=40%>
- The cluster assignment hardening loss enforces making these soft assignment probabilities sticter.
  - Cluster assignment probability distribution Q approach an auxiliary distribution P which guarantees this constraint.
    <img src="https://latex.codecogs.com/svg.latex?\Large&space;p_{ij}=\frac{q^2_{ij}/\Sigma_{i}q_{ij}}{\Sigma_{j'}(q^2_{ij'}/\Sigma_{i}q_{ij'})}" width=25%>
- Formulate the divergence between the two probability distributions (ex. KL-divergence)

#### Balanced assignments loss
- Enforce having balanced cluster assignments.
  <img src="https://latex.codecogs.com/svg.latex?\Large&space;L_{ba}=KL(G||U)" width=20%>
  ,where U is the uniform distribution and G is the probability distribution of assigning a point to each cluster:
  <img src="https://latex.codecogs.com/svg.latex?\Large&space;g_k=P(y=k)=\frac{1}{N}\Sigma_iq_ik" width=35%>

#### Locality-preserving loss
<img src="https://latex.codecogs.com/svg.latex?\Large&space;L_{lp}=\Sigma_i\Sigma_{j\text{in}N_k(i)}s(x_i,x_j)||z_i-z_j||^2" width=45%>
,where N<sub>k</sub>(i) is the set of k nearest neighbors of the data point x<sub>i</sub>, and s(x<sub>i</sub>,x<sub>j</sub>) is a similarity measure between the point x<sub>i</sub> and x<sub>j</sub>.

#### Group Sparsity loss
- The hidden units were divided into G groups, where G is the assumed number of clusters. 
    <img src="https://latex.codecogs.com/svg.latex?\Large&space;L_{gs}=\Sigma^N_{i=1}\Sigma^G_{g=1}\lambda_g||\Phi^g(x_i)||" width=40%>

#### Cluster classification loss

#### Agglomerative clustering loss

### Method to combine the losses
<img src="https://latex.codecogs.com/svg.latex?\Large&space;L(\theta)={\alpha}L_c(\theta)+(1-\alpha)L_n(\theta)" width=40%>
,where L<sub>c</sub>(&theta;) is the clustering loss, L<sub>n</sub>(&theta;) si the non-clustering loss, and &alpha; is in [0,1]. 

- The following are methods to assign and schedule the values of &alpha;.
  - **Pre-training**, fine-tuning : First, &alpha; is set to 0 (the network is trained using the non-clustering loss only). Subsequently, &alpha; is set to 1 (the non-clustering network branches are removed and the clustering loss is used to train the obtained network)
  - **Joint training** : 0 < &alpha; < 1
  - **Variable schedule** : &alpha; is varied during the training dependent on a chosen schedule.

### Cluster updates
- Jointly updated with the network model
- Alternatingly updated with the network model

### After network training
- Clustering a similar dataset
- Obtaining better result

### Validation metrics
- Accuracy (ACC), normalized mutual information (NMI) in [0,1]